{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d428b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b85be267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU \n",
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "epochs, batch_size  = 100, 64\n",
    "lr, b1, b2 = 2e-4, 0.5, 0.999\n",
    "latent_dim = 100\n",
    "img_size = 28\n",
    "channels = 1\n",
    "n_critic = 5\n",
    "lambda_gp = 10\n",
    "img_shape = (channels, img_size, img_size)\n",
    "if torch.cuda.is_available(): \n",
    "    print(\"Train on GPU \\nCUDA is available\")\n",
    "    cuda = True \n",
    "else:\n",
    "    print(\"Train on the CPU \\nCUDA is not available\")\n",
    "    cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1891b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize(img_size), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437dfea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0b7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        flat_img = img.view(img.shape[0], -1)\n",
    "        pred = self.model(flat_img)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc200e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    G.cuda()\n",
    "    D.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8fb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "def gradient_penalty(D, real_img, fake_img):\n",
    "    alpha = Tensor(np.random.random((real_img.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha * real_img + ((1 - alpha) * \n",
    "                                        fake_img)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_img.shape[0], 1).fill_(1.0)\n",
    "                    , requires_grad=False)\n",
    "    gradients = autograd.grad(outputs=d_interpolates,\n",
    "                              inputs=interpolates,\n",
    "                              grad_outputs=fake,\n",
    "                              create_graph=True,\n",
    "                              retain_graph=True,\n",
    "                              only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    GP = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa5aa17",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmean(real_pred) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(fake_pred) \u001b[38;5;241m+\u001b[39m lambda_gp \u001b[38;5;241m*\u001b[39m GP\n\u001b[0;32m     19\u001b[0m loss_d\u001b[38;5;241m.\u001b[39mappend(d_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 20\u001b[0m d_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(b1, b2))\n",
    "current_iters = 0\n",
    "os.makedirs(\"WGAN-GP_results\", exist_ok=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        \n",
    "        ## Train Discriminator ##\n",
    "        optimizer_D.zero_grad()\n",
    "        z = Variable(Tensor(np.random.normal(0, 1,\n",
    "                                             (imgs.shape[0], latent_dim))))\n",
    "        fake_imgs = G(z)\n",
    "        real_pred = D(real_imgs)\n",
    "        fake_pred = D(fake_imgs)\n",
    "        GP = gradient_penalty(D, real_imgs.data, fake_imgs.data)\n",
    "        d_loss = -torch.mean(real_pred) + torch.mean(fake_pred) + lambda_gp * GP\n",
    "        loss_d.append(d_loss.item())\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        if i % n_critic == 0:\n",
    "            ## Train Generator ##\n",
    "            fake_imgs = G(z)\n",
    "            fake_pred = D(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_pred)\n",
    "            loss_g.append(g_loss.item())\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            current_iters += n_critic\n",
    "    print(\"[Epoch %d/%d]  [D loss: %f] [G loss: %f]\"\n",
    "          % (epoch, epochs, d_loss.item(), g_loss.item()))\n",
    "    save_image(fake_imgs.data[:25], \"WGAN-GP_results/epoch_%d.png\" \n",
    "               % epoch, nrow=5, normalize=True)\n",
    "    \n",
    "torch.save(G.state_dict(), './Generator.pth')\n",
    "torch.save(D.state_dict(), './Discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce13cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision2",
   "language": "python",
   "name": "vision_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
