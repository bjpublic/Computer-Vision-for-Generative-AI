{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Diffusion Probabilistic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_steps = 1000\n",
    "max_epoch = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class Data_setting(Dataset):\n",
    "    def __init__(self, train):\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        train_dataset = CIFAR10(\n",
    "            \"./data\", download=True, train=train, transform=transform)\n",
    "        self.dataset_len = len(train_dataset.data)\n",
    "        data = torch.Tensor(train_dataset.data)\n",
    "        self.depth = 3\n",
    "        self.size = 32\n",
    "        self.input_seq = ((data / 255.0) * 2.0) - 1.0\n",
    "        self.input_seq = self.input_seq.moveaxis(3, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.input_seq[item]\n",
    "    \n",
    "train_dataset = Data_setting(True)\n",
    "val_dataset = Data_setting(False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          num_workers=0, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                        num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.mha = nn.MultiheadAttention(h_size, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value\n",
    "\n",
    "class SAWrapper(nn.Module):\n",
    "    def __init__(self, h_size, num_s):\n",
    "        super(SAWrapper, self).__init__()\n",
    "        self.sa = nn.Sequential(*[SelfAttention(h_size) for _ in range(1)])\n",
    "        self.num_s = num_s\n",
    "        self.h_size = h_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.h_size, self.num_s * self.num_s).swapaxes(1, 2)\n",
    "        x = self.sa(x)\n",
    "        x = x.swapaxes(2, 1).view(-1, self.h_size, self.num_s, self.num_s)\n",
    "        return x\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, in_channels, residual=True)\n",
    "            self.conv2 = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(pl.LightningModule):\n",
    "    def __init__(self, in_size, t_range, img_depth):\n",
    "        super().__init__()\n",
    "        self.beta_small = 1e-4\n",
    "        self.beta_large = 0.02\n",
    "        self.t_range = t_range\n",
    "        self.in_size = in_size\n",
    "\n",
    "        bilinear = True\n",
    "        self.inc = DoubleConv(img_depth, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = Down(256, 512 // factor)\n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, img_depth)\n",
    "        self.sa1 = SAWrapper(256, 8)\n",
    "        self.sa2 = SAWrapper(256, 4)\n",
    "        self.sa3 = SAWrapper(128, 8)\n",
    "\n",
    "    def pos_encoding(self, t, channels, embed_size):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device)\n",
    "                .float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc.view(-1, channels, 1, 1).repeat(1, 1, embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) + self.pos_encoding(t, 128, 16)\n",
    "        x3 = self.down2(x2) + self.pos_encoding(t, 256, 8)\n",
    "        x3 = self.sa1(x3)\n",
    "        x4 = self.down3(x3) + self.pos_encoding(t, 256, 4)\n",
    "        x4 = self.sa2(x4)\n",
    "        x = self.up1(x4, x3) + self.pos_encoding(t, 128, 8)\n",
    "        x = self.sa3(x)\n",
    "        x = self.up2(x, x2) + self.pos_encoding(t, 64, 16)\n",
    "        x = self.up3(x, x1) + self.pos_encoding(t, 64, 32)\n",
    "        output = self.outc(x)\n",
    "        return output\n",
    "\n",
    "    def beta(self, t):\n",
    "        return self.beta_small + (t / self.t_range) * (\n",
    "            self.beta_large - self.beta_small\n",
    "        )\n",
    "\n",
    "    def alpha(self, t):\n",
    "        return 1 - self.beta(t)\n",
    "\n",
    "    def alpha_bar(self, t):\n",
    "        return math.prod([self.alpha(j) for j in range(t)])\n",
    "\n",
    "    def get_loss(self, batch, batch_idx):\n",
    "        ts = torch.randint(0, self.t_range, [batch.shape[0]], device=self.device)\n",
    "        noise_imgs = []\n",
    "        epsilons = torch.randn(batch.shape, device=self.device)\n",
    "        for i in range(len(ts)):\n",
    "            a_hat = self.alpha_bar(ts[i])\n",
    "            noise_imgs.append(\n",
    "                (math.sqrt(a_hat) * batch[i]) + (math.sqrt(1 - a_hat) * epsilons[i])\n",
    "            )\n",
    "        noise_imgs = torch.stack(noise_imgs, dim=0)\n",
    "        e_hat = self.forward(noise_imgs, ts.unsqueeze(-1).type(torch.float))\n",
    "        loss = nn.functional.mse_loss(\n",
    "            e_hat.reshape(-1, self.in_size), epsilons.reshape(-1, self.in_size)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def denoise_sample(self, x, t):\n",
    "        with torch.no_grad():\n",
    "            if t > 1:\n",
    "                z = torch.randn(x.shape)\n",
    "            else:\n",
    "                z = 0\n",
    "            e_hat = self.forward(x, t.view(1, 1).repeat(x.shape[0], 1))\n",
    "            pre_scale = 1 / math.sqrt(self.alpha(t))\n",
    "            e_scale = (1 - self.alpha(t)) / math.sqrt(1 - self.alpha_bar(t))\n",
    "            post_sigma = math.sqrt(self.beta(t)) * z\n",
    "            x = pre_scale * (x - e_scale * e_hat) + post_sigma\n",
    "            return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.get_loss(batch, batch_idx)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.get_loss(batch, batch_idx)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = DiffusionModel(train_dataset.size*train_dataset.size,\n",
    "                       diffusion_steps, train_dataset.depth)\n",
    "\n",
    "tb_logger = pl.loggers.TensorBoardLogger(\n",
    "    \"lightning_logs/\",\n",
    "    name='DDPM_test',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epoch, \n",
    "    log_every_n_steps=10, \n",
    "    devices=1,\n",
    "    logger=tb_logger\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_shape = [3, 3]\n",
    "sample_batch_size = gif_shape[0] * gif_shape[1]\n",
    "n_hold_final = 10\n",
    "\n",
    "gen_samples = []\n",
    "x = torch.randn((sample_batch_size, train_dataset.depth, train_dataset.size, train_dataset.size))\n",
    "sample_steps = torch.arange(model.t_range-1, 0, -1)\n",
    "for t in sample_steps:\n",
    "    x = model.denoise_sample(x, t)\n",
    "    if t % 50 == 0:\n",
    "        gen_samples.append(x)\n",
    "for _ in range(n_hold_final):\n",
    "    gen_samples.append(x)\n",
    "gen_samples = torch.stack(gen_samples, dim=0).moveaxis(2, 4).squeeze(-1)\n",
    "gen_samples = (gen_samples.clamp(-1, 1) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_samples = (gen_samples * 255).type(torch.uint8)\n",
    "gen_samples = gen_samples.reshape(-1, gif_shape[0], \n",
    "                                  gif_shape[1], train_dataset.size, train_dataset.size, train_dataset.depth)\n",
    "\n",
    "def stack_samples(gen_samples, stack_dim):\n",
    "    gen_samples = list(torch.split(gen_samples, 1, dim=1))\n",
    "    for i in range(len(gen_samples)):\n",
    "        gen_samples[i] = gen_samples[i].squeeze(1)\n",
    "    return torch.cat(gen_samples, dim=stack_dim)\n",
    "\n",
    "gen_samples = stack_samples(gen_samples, 2)\n",
    "gen_samples = stack_samples(gen_samples, 2)\n",
    "\n",
    "imageio.mimsave(\n",
    "    f\"{trainer.logger.log_dir}/pred.gif\",\n",
    "    list(gen_samples),\n",
    "    fps=5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f961cc1adb1d23b163d13487a7bbea047622fac31372ac86f89c77664d2955"
  },
  "kernelspec": {
   "display_name": "vision3",
   "language": "python",
   "name": "vision_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
