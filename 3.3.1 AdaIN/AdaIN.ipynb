{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787cd7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG, decoder 모델 다운로드 링크\n",
    "# drive.google.com/drive/folders/12g2eYD4oqd8F269nFlZ0qNEUEUGxcCPe?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09164e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import net\n",
    "from function import calc_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9accd800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_instance_normalization(content_feat, style_feat):\n",
    "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
    "    size = content_feat.size()\n",
    "    style_mean, style_std = calc_mean_std(style_feat)\n",
    "    content_mean, content_std = calc_mean_std(content_feat)\n",
    "\n",
    "    adain_feat = (content_feat - content_mean.expand(\n",
    "        size)) / content_std.expand(size)\n",
    "    return adain_feat * style_std.expand(size) + style_mean.expand(size)\n",
    "\n",
    "def test_transform(size, crop):\n",
    "    transform_list = []\n",
    "    if size != 0:\n",
    "        transform_list.append(transforms.Resize(size))\n",
    "    if crop:\n",
    "        transform_list.append(transforms.CenterCrop(size))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "\n",
    "def style_transfer(vgg, decoder, content, style, alpha=1.0):\n",
    "    assert (0.0 <= alpha <= 1.0)\n",
    "    content_f = vgg(content)\n",
    "    style_f = vgg(style)\n",
    "    \n",
    "    feat = adaptive_instance_normalization(content_f, style_f)\n",
    "    feat = feat * alpha + content_f * (1 - alpha)\n",
    "    return decoder(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c56196",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = './input/content/lenna.jpg'\n",
    "style_path = './input/style/asheville.jpg'\n",
    "vgg_path = 'models/vgg.pth'\n",
    "decoder_path = 'models/decoder.pth'\n",
    "content_size = 512\n",
    "style_size = 512\n",
    "crop = False\n",
    "output = './output'\n",
    "alpha = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42dbb6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (2): ReLU()\n",
       "  (3): Upsample(scale_factor=2.0, mode='nearest')\n",
       "  (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (6): ReLU()\n",
       "  (7): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (9): ReLU()\n",
       "  (10): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (12): ReLU()\n",
       "  (13): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (14): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (15): ReLU()\n",
       "  (16): Upsample(scale_factor=2.0, mode='nearest')\n",
       "  (17): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (18): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (19): ReLU()\n",
       "  (20): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (21): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (22): ReLU()\n",
       "  (23): Upsample(scale_factor=2.0, mode='nearest')\n",
       "  (24): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (25): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (26): ReLU()\n",
       "  (27): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (28): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "output_dir = Path(output)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "assert (content_path or style_path)\n",
    "content_path = Path(content_path)\n",
    "style_path = Path(style_path)\n",
    "\n",
    "decoder = net.decoder\n",
    "decoder.eval()\n",
    "\n",
    "vgg = net.vgg\n",
    "vgg.eval()\n",
    "\n",
    "decoder.load_state_dict(torch.load(decoder_path))\n",
    "vgg.load_state_dict(torch.load(vgg_path))\n",
    "vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "\n",
    "vgg.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cbd91ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tf = test_transform(content_size, crop)\n",
    "style_tf = test_transform(style_size, crop)\n",
    "\n",
    "content = content_tf(Image.open(content_path))\n",
    "style = style_tf(Image.open(style_path))\n",
    "\n",
    "style = style.to(device).unsqueeze(0)\n",
    "content = content.to(device).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    output = style_transfer(vgg, decoder, content, style, alpha)\n",
    "output = output.cpu()\n",
    "\n",
    "output_name = output_dir / '{:s}_stylized_{:s}.jpg'.format(\n",
    "    content_path.stem, style_path.stem)\n",
    "save_image(output, str(output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a0e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adain",
   "language": "python",
   "name": "adain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
